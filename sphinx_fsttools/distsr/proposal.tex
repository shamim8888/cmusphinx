\documentclass{article}
\title{Scalable Multi-Pass Speech Recognition for Low-Power Networks of Heterogeneous Devices} 
\begin{document}
\begin{abstract}

  With the advent of smartphones and other networked portable
  computing devices, it has become commonplace for people to have
  several ``personal computers'' with which they interact on a daily
  basis.  As part of our ongoing research into note-taking and
  personal information managment using speech, we would like to be
  able to collect, process, and access information extracted from
  speech data on all of these various devices.  Unfortunately,
  portable devices are highly constrained by considerations of size,
  cost, and power consumption, and are not capable of performing
  large-vocabulary continuous speech recognition and information
  extraction.  As a step towards a fully distributed personal
  knowledge management system, I propose a distributed approach to
  multi-pass speech recognition, where at each pass, information is
  extracted on each device at a level of processing commensurate with
  its abilities, and stored in a compact form suitable for
  synchronization over low-power personal area networks.  Contrary to
  previous approaches to distributed speech recognition, this method
  does not presume the existence of a centralized compute server.
  Furthermore, I will also demonstrate the suitability of this
  approach for accelerating speech recognition on asymmetric
  multiprocessors.
  
\end{abstract}

\section{Automatic Speech Recognition}
\label{sec:asr}

Applications

Problems

\subsection{Feature Extraction}
\label{sec:fe}

Source-filter model of speech production

Autoregressive (LPC) Analysis

Cepstral Analysis

Mel-frequency Cepstral Coefficients

Fixed-Point Implementation of MFCC

Feature Normalization

\subsection{Acoustic Modeling}
\label{sec:am}

Hidden Markov Models

Modeling output probabilities with Gaussian Mixture Models

State and Mixture tying

Dimensionality Reduction - PCA, LDA, MLLT

Speaker Adaptive Modeling - MAP, MLLR

Model compression and quantization - SDCHMM, qHMM

\subsection{Language Modeling}
\label{sec:lm}

Finite-State Grammars

History-Based (N-Gram) Statistical Language Models

Model compression and quantization

\subsection{Search}
\label{sec:search}

Time-Synchronous Viterbi Beam Search

Lattice Rescoring and Multi-Pass Search

Previous work in parallel search - moderately and massively parallel decoders

\section{Distributed Computing}
\label{sec:pdc}

Two instances of the same problem - network of heterogeneous devices,
asymmetric multiprocessor.

\subsection{Distributed Computing Architectures}
\label{sec:arch}

Taxonomy of computer architectures - SISD, SIMD, MIMD

Topologies of distributed computing projects - hierarchical or peer-to-peer

Because of the problems of limited network bandwidth and latency and
the need to access information as quickly as possible, this thesis
proposes a peer-to-peer approach to distributed speech recognition.

\subsection{Challenges for Distributed ASR}
\label{sec:challenges}

``Distributed Speech Recognition'' is not the subject of this thesis,
and it doesn't work particularly well anyway.

Relative percentage of time spent in FE, GMM, HMM, LM varies with
vocabulary size.

Serial dependencies in search

\subsection{Distributed Revision Control}

The paradigm used for distributed speech recognition in this thesis is
based on the {\em distributed revision control} systems currently
becoming popular in the software development world.

Traditional revision control systems such as SourceSafe require all
users to work within a single shared source tree.  In order to work on
a file, it is necessary to ``check out'' that file, which places a
lock on it preventing others from modifying it.  While this model
works reasonably well when all developers are in close proximity to
each other, it becomes unmanageable when developers are distributed
among different organizations and locations.

The Concurrent Versions System (CVS) introduced the separation between
the {\em repository} and the {\em working copy}.  Each developer has
their own working copy in which they can make changes.  Before
committing, they must synchronize their copy with the repository and
resolve any conflicts.  This process of synchronization is known as
{\em merging} with the repository.  Since developers are free to make
their own changes locally without preventing others from working, this
model scales much better to large, widely-distributed development
teams.  Virtually every major open-source software project has used
CVS, or its descendant Subversion, at one point.

Frustration with many of the limitations of CVS led to the development
of Subversion, which can be thought of as ``CVS done right''.  One of
the many problems with CVS is that when dealing with remote
repositories, every operation requires a connection to the server,
thus making it impossible to perform even simple operations such as
``diff'' and ``revert'' without a network connection.  Subversion
deals with this problem by caching the base revision locally, so that
checking for or undoing local modifications can be done without
contacting the repository.

There are several lingering problems with the centralized repository
approach to revision control.  Since projects typically enforce some
policy with regard to the quality of checkins to the main repository,
developers will often delay committing changes.  While it is clearly
desirable for the main repository to contain working code, this can
lead to rather large commits which may fix multiple bugs or introduce
multiple features.  The history of changes made to the working copy is
lost.  Likewise, when a developer updates from the repository, merging
it with their local copy, the changes made locally may be overwritten,
and are lost forever.

Distributed revision control systems address these problems by erasing
the distinction between the repository and the working copy.  With a
distributed revision control system, instead of ``checking out'' a
working copy, a developer ``clones'' the main repository, producing a
local branch.  The developer can then commit changes to this branch
without affecting the main repository.  Then, when it's time to commit
these changes, the developer ``pulls'' updates from the main
repository, merges their local changes, and then ``pushes'' any
updates back.  This workflow solves both of the problems mentioned
above - disconnected operation is possible (even committing changes
can be done offline), and the local revision history is not lost.

In addition to allowing disconnected operation and removing the effect
of network latency on simple operations, this type of revision control
can be more robust, since it removes the single point of failure in
centralized systems.

\section{Preliminary Work}
\label{sec:prelim}

Compression of semi-continuous models

DSP implementation of front-end and GMM computation

\section{Proposed Work}
\label{sec:proposed}

I propose to recast the problem of automatic speech recognition as a
system of distributed {\em annotators} which operate on a structured
representation of speech.  The operation by which these annotators
combine their knowledge is a {\em merging} operation.  This is the
same principle as used by distributed revision control systems, except
that:

\begin{itemize}
\item The modifications and additions in this system are often
  automatically generated (but need not be).
\item The merging operation is {\em probabilistic}.  A revision
  control system relies on human input to resolve conflicts, and
  expects conflicts to have a unique resolution.  In this system,
  conflicts have a ``soft'' resolution in favor of the MAP hypothesis
  given the current state of knowledge.
\end{itemize}


\section{Evaluation Metrics}
\label{sec:eval}

\section{Contributions and Time-Line}
\label{sec:contrib}

\bibliographystyle{apalike}
\bibliography{asrtts}

\end{document}
