\documentclass{article}
\title{Scalable Speech Recognition Using Distributed Annotators} 
\author{David Huggins-Daines}
\begin{document}
\maketitle

\begin{abstract}


\end{abstract}

\tableofcontents{}

\section{Introduction}
\label{sec:intro}

As Moore's Law continues to give us more and smaller transistors,
computers as we know them are undergoing a radical transformation.  We
are currently witnessing the nascent ubiquity of intelligent mobile
devices; for instance, the cellular phones of today are more capable
in terms of raw processor speed than the personal computers of a mere
10 years ago.  Many people are now rarely outside an arm's reach of
two or more fully programmable, multimedia capable and Internet
connected devices, be they smartphones, laptops, or conventional PCs
and workstations.  Wireless networks, too, have become ubiquitous, as
they provide a convenient way both to access Internet content and move
data between mobile devices.

In some respects, however, the growth curve of computational capacity
has begun to flatten.  The single-threaded performance of modern CPUs
has mostly stopped increasing, as clock speeds have levelled off and
gains in instruction-level parallelism are becoming increasingly hard
to come by.  In addition, as more and more interesting applications
rely on Internet access, network latency rather than processor speed
has become a deciding factor in application performance.  Moreover,
both processor clock speed and network latency are strongly
constrained by the physics of semiconductors and fiber-optic cables,
respectively.  This means that on the local node, performance gains
will primarily come from increased thread-level parallelism, while
over the network, performance gains will be achieved by distributing
the processing and storage of data away from centralized servers and
closer to the user.

What do these trends mean for the future of automatic speech
recognition?  This thesis is an attempt to answer this question, by
proposing an approach to speech recognition based on the concept of
lightweight, independent {\em annotators} which collaborate, either
sequentially or in parallel, on a structured representation of speech.
The key motivation behind this approach is {\em scalability} - the
system proposed here will adapt to the available computing resources
without retraining, and as more resources become available, the
performance of the system will improve.  This scalability will be
achieved through increasing the {\em diversity} of models and search
strategies, rather than the standard approach of increasing their {\em
  intensity}.

In the next section, we review the state of the art in automatic
speech recognition, and give a description of the system used in this
thesis proposal.  We then proceed to a description of previous
approaches to parallel recognition and system combination.  The
following sections detail the completed and proposed work in this
thesis.  We conclude with a statement of the contributions of this
thesis and a proposed timeline for completing them.

\section{Automatic Speech Recognition}
\label{sec:asr}

Automatic speech recognition, hereafter referred to as ASR, is the
identification of linguistic content (words, phrases, or concepts) in
audio data without human intervention.  While a broad range of
applications have been constructed or proposed based on this
technology, ASR systems can generally be grouped into four main tasks,
which are presented here in roughly increasing order of difficulty:

\begin{itemize}
\item {\em Voice control} - Use of voice commands, typically from a
  single user and directed at the system, to control a computer or
  some application running on it.
\item {\em Dictation} - Conversion of speech, typically from a single
  user and directed at the system, to text, with control components to
  allow for editing and error correction.
\item {\em Dialog} - Speech-based interaction between a computer and a
  human, whose purpose is to achieve some external goal.
\item {\em Transcription} - Conversion of speech, typically from
  multiple users and not directed at the system, to text.
\end{itemize}

Of these, the {\em dictation} and {\em dialog} tasks are the ones
which specifically require good interactive performance.  We also
believe that these are the ones which are most relevant to handheld
and mobile devices in the future.

Regardless of the task or domain, nearly all current ASR systems share
a common architecture, which is presented visually in Figure
\ref{fig:asrsys}.  The components of this architecture are discussed
in greater detail in the following subsections.

\subsection{Acoustic Modeling}
\label{sec:am}

The principal question in building an ASR system is how to represent
the underlying entities which we are attempting to recognize.  ASR is
fundamentally a {\em classification} problem, and thus recognition
consists, at some level, of matching an input against a set of
reference patterns and classifying it as ``belonging'' to one class or
another.  However, due to the non-stationary nature of speech, ASR is
also a {\em sequence matching} problem.

\subsubsection{Dynamic Time Warping}
\label{sec:dtw}

One of the most successful early approaches to ASR is the {\em dynamic
  time warping} algorithm, first described in \cite{itakura1975}

\subsubsection{Hidden Markov Models}
\label{sec:hmm}

Hidden Markov Models (hereafter referred to as HMMs) are a
mathematical formalism for pattern recognition, first desribed in
\cite{baum1966}, which have proven to be extremely useful in modeling
speech, both for recognition \cite{rabiner1989} and for synthesis
\cite{masuko1996}.

In HMM-based recognition, rather than having a template against which
incoming speech is matched directly, we construct a probabilistic
model of the underlying speech generation process, and evaluate
incoming speech against this model.  As with all generative models of
speech production (e.g. the source-filter model), an HMM is a highly
simplified and abstract version of a very complicated process.
However, in practice, it captures enough of the structure of speech to
be useful.  Estimation and inference on HMMs is also computationally
efficient.

\subsection{Language Modeling}
\label{sec:ngrams}

\subsection{Acoustic Feature Calculation}
\label{sec:fe}

The purpose of acoustic feature calculation is to transform the speech
signal into a numerical representation appropriate for pattern
recognition.  The input signal is nearly always represented in a
sampled and quantized time-domain form, using pulse code modulation or
some variant thereof.  This representation is unsuitable for automatic
speech recognition for several reasons.  The most important is that
the human perceptual system distinguishes between classes of speech
sounds primarily through coarse distinctions in frequency domain, and
therefore a spectral representation is more useful.  Also, the raw
speech signal is non-stationary and therefore cannot be meaningfully
analyzed as a whole.  Therefore, it must be segmented into smaller
units.

The approach used by most current systems to deal with the
non-stationarity of speech is to segment the audio into evenly spaced
frames.  The frame spacing used is usually 10 milliseconds, which is
short enough to capture the rapid transitions present in some speech
sounds.  The choice of 10 millisecond shifts is arbitrary and
represents a compromise between achieving sufficient time-domain
resolution and avoiding redundant computation.  Variable frame rates
have been tried with some success, particularly in noisy conditions
\cite{zhu2000}.  The length of each frame is chosen so as to give
sufficient resolution in frequency domain, and is typically about 25
milliseconds.

After segmenting the audio, the individual frames are analyzed to
produce a suitable feature vector for classification.  The goal of
this analysis is to produce features which contain only the
information relevant for classifying speech sounds, and which are
robust to variations in the recording channel, environmental noise,
and speaker variability.

Source-filter model - feature extraction for ASR attempts to model the
filter while removing the effects of the source.  The two most popular
ways of doing this are {\em linear prediction} and {\em cepstral
  analysis}.

\subsubsection{Linear Prediction}
\label{sec:lpc}

Linear predictive analysis (referred to as LPC from here on) is based
on the idea that each speech sample can be predicted as a linear
combination of the preceding samples.  Why this is useful for speech
recognition is that LPC also results in a compact representation of
the coarse power spectrum.

In current systems, LPC is typically used as part of the PLP
\cite{hermansky1990} and RASTA \cite{hermansky1994} algorithms.

\subsubsection{Cepstral Analysis}
\label{sec:ceps}

Cepstral coefficients have a useful property which improves their
robustness to channel variability.  Assuming the channel can be
modeled as a linear system, its effect on the signal in time domain
can be expressed as a convolution of the ``pure'' signal with a
channel-specific ``noise'' signal.  The effect of the logarithm and
the inverse transform in cepstral analysis is to convert this
convolution into a sum of the ``pure'' cepstrum and the ``noise''
cepstrum.  Therefore, to remove the effect of the channel, we can
simply subtract out the noise cepstrum.  This noise cepstrum can be
estimated as the long-term average of the cepstrum, a technique known
as {\em cepstral mean subtraction}.  If the long-term cepstrum is not
available or does not produce a reliable estimate, local methods can
be used, such as {\em codeword-dependent cepstral normalization}
\cite{acero1990}.

\subsubsection{Discriminative Feature Transforms}
\label{sec:lda}

While LPC and cepstral features are optimal with respect to the
source-filter model, this model is only loosely related to the problem
of distinguishing between classes of speech sounds.  It has therefore
become popular to apply transformations to these features in order to
decorrelate the individual components of the feature vector and to
maximize the separability of acoustic classes in vector space.

Principal Components Analsys (PCA).

Linear Discriminant Analysis (LDA), also known as Fisher's Linear
Discriminant.  Heteroscedastic Linear Discriminant Analysis (HLDA).

MLLT \cite{gopinath1998} is another linear transformation which was
designed to compensate for the error introduced by using multivariate
Gaussians with diagonal covariance matrices to model speech sounds,
which will be discussed further in Section \ref{sec:gmm}.  While in
fact this is simply a variety of HLDA, empirically it has been shown
to be useful as a postprocessing step after applying a discriminative
transform.

\subsection{Observation Density Calculation}
\label{sec:gmm}

In most modern HMM-based speech recognition systems, the state
observation probabilities are modeled using mixtures of multivariate
Gaussian distributions.

\subsection{Search}
\label{sec:search}

\subsubsection{Lexicon Tree Search}
\label{sec:lextree}

Static lexicon tree search (old Sphinx papers, Ney 1992?)

Word-conditioned lexicon tree search \cite{kanthak2000}

\subsubsection{Finite State Transducer Search}
\label{sec:fst}

WFST search \cite{mohri2000}

Comparison of WFST and tree search \cite{kanthak2002}

Saon, Povey, and Zweig ``extremely fast decoder'' \cite{saon2006}

\subsubsection{Word Lattice Generation}
\label{sec:lattice}

Hermann Ney paper on word graphs

Saon, Povey, and Zweig redux

On-line lattice generation \cite{sagerer1996}

\section{The PocketSphinx Speech Recognition System}
\label{sec:pocketsphinx}

\subsection{Design Goals}
\label{sec:design}

Simplicity of implementation

Reasonable default settings

Small code and data footprint

Memory efficiency


\section{Parallel and Distributed ASR}
\label{sec:pdasr}

Ravi's really old paper about parallel ASR \cite{mosur1993}

Phillips parallel recognizer \cite{phillips1999}

NEC paper about block-based phoneme lookahead on cellphones \cite{ishikawa2006}

Japanese paper about massively parallel decoding \cite{shinozaki2004}

\section{Hypothesis and System Combination}
\label{sec:hypcomb}

ROVER \cite{fiscus1997}

Rita Singh and Xiang Li's lattice combination \cite{li2002}

Consensus network based combination \cite{mangu2000}

\section{Preliminary Work}
\label{sec:prelim}

\subsection{Pipelined Multi-Pass Search}
\label{sec:pipeline}

It was suggested, though not implemented, in \cite{mosur1996} that the
multiple passes of search used in the Sphinx decoder could be
pipelined and run in separate threads.

\subsection{Incremental Lattice Combination}
\label{sec:incrlat}


\section{Proposed Work}
\label{sec:proposed}

Finite State Transducer search for PocketSphinx

Architecture for Parallel and Distributed decoding with annotators

Lattice combination between heterogeneous systems

Probabilistic lattice combination

\section{Evaluation Metrics}
\label{sec:eval}

Metric for measuring speed-accuracy tradeoff

\section{Contributions and Timeline}
\label{sec:contrib}

\begin{itemize}
\item We will extend the concept of word lattice combination to
  heterogeneous systems with prior probabilities.
\item We will formulate a probabilistic merging operation over word
  lattices which subsumes existing fast-match and multi-pass search
  algorithms.
\item We will demonstrate that this approach scales across multiple
  CPU cores as well as multiple networked devices, as measured by the
  optimal speed-accuracy tradeoff achieved.
\end{itemize}

\bibliographystyle{plain}
\bibliography{proposal}

\end{document}
