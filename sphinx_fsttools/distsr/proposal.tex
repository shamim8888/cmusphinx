\documentclass{article}
\title{Scalable Speech Recognition Using Distributed Annotators} 
\begin{document}
\begin{abstract}

  With the advent of smartphones and other networked portable
  computing devices, it has become commonplace for people to have
  several ``personal computers'' with which they interact on a daily
  basis.  As part of our ongoing research into note-taking and
  personal information managment using speech, we would like to be
  able to collect, process, and access information extracted from
  speech data on all of these various devices.  Unfortunately,
  portable devices are highly constrained by considerations of size,
  cost, and power consumption, and are not capable of performing
  large-vocabulary continuous speech recognition and information
  extraction.  As a step towards a fully distributed personal
  knowledge management system, I propose a distributed approach to
  multi-pass speech recognition, where at each pass, information is
  extracted on each device at a level of processing commensurate with
  its abilities, and stored in a compact form suitable for
  synchronization over low-power personal area networks.  Contrary to
  previous approaches to distributed speech recognition, this method
  does not presume the existence of a centralized compute server.  In
  addition, these distributed annotators will be capable of operating
  time-synchronously to accelerate speech recognition on
  multiprocessing systems.
  
\end{abstract}

\section{Automatic Speech Recognition}
\label{sec:asr}

Applications

Problems

\subsection{Feature Extraction}
\label{sec:fe}

Source-filter model of speech production

Autoregressive (LPC) Analysis

Cepstral Analysis

Mel-frequency Cepstral Coefficients

Fixed-Point Implementation of MFCC

Feature Normalization

\subsection{Acoustic Modeling}
\label{sec:am}

Hidden Markov Models

Modeling output probabilities with Gaussian Mixture Models

State and Mixture tying

Dimensionality Reduction - PCA, LDA, MLLT

Speaker Adaptive Modeling - MAP, MLLR

Model compression and quantization - SDCHMM, qHMM

\subsection{Language Modeling}
\label{sec:lm}

Finite-State Grammars

History-Based (N-Gram) Statistical Language Models

Model compression and quantization

\subsection{Search}
\label{sec:search}

Time-Synchronous Viterbi Beam Search

Lattice Rescoring and Multi-Pass Search

Previous work in parallel search - moderately and massively parallel decoders

\section{Distributed Computing}
\label{sec:pdc}

Two instances of the same problem - network of heterogeneous devices,
asymmetric multiprocessor.

\subsection{Distributed Computing Architectures}
\label{sec:arch}

Taxonomy of computer architectures - SISD, SIMD, MIMD

Topologies of distributed computing projects - hierarchical or peer-to-peer

Because of the problems of limited network bandwidth and latency and
the need to access information as quickly as possible, this thesis
proposes a peer-to-peer approach to distributed speech recognition.

\subsection{Challenges for Distributed ASR}
\label{sec:challenges}

``Distributed Speech Recognition'' is not the subject of this thesis,
and it doesn't work particularly well anyway.

Relative percentage of time spent in FE, GMM, HMM, LM varies with
vocabulary size.

Serial dependencies in search

\section{Preliminary Work}
\label{sec:prelim}

Compression of semi-continuous models

Pipelined multi-pass search

\section{Proposed Work}
\label{sec:proposed}

I propose to recast the problem of automatic speech recognition as a
system of distributed {\em annotators} which operate on a structured
representation of speech.  The operation by which these annotators
combine their knowledge is a {\em merging} operation.  This is the
same principle as used by distributed revision control systems, except
that:

\begin{itemize}
\item The modifications and additions in this system are often
  automatically generated (but need not be).
\item The merging operation is {\em probabilistic}.  A revision
  control system relies on human input to resolve conflicts, and
  expects conflicts to have a unique resolution.  In this system,
  conflicts have a ``soft'' resolution in favor of the MAP hypothesis
  given the current state of knowledge.
\end{itemize}


\section{Evaluation Metrics}
\label{sec:eval}

\section{Contributions and Time-Line}
\label{sec:contrib}

\bibliographystyle{apalike}
\bibliography{asrtts}

\end{document}
