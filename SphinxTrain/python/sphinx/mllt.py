"""Train maximum-likelihood linear transforms.

This module implements the MLLT technique as described in
R. A. Gopinath, "Maximum Likelihood Modeling with Gaussian
Distributions for Classification", in proceedings of ICASSP 1998.
"""

# Copyright (c) 2006 Carnegie Mellon University
#
# You may copy and modify this freely under the same terms as
# Sphinx-III

__author__ = "David Huggins-Daines <dhuggins@cs.cmu.edu>"
__version__ = "$Revision$"

from numpy import dot, prod, diag, log, eye
from numpy.random import random
from numpy.linalg import det, inv
from scipy.optimize import fmin_bfgs

import s3lda

class MLLTModel(object):
    """Train MLLT (maximum likelihood linear transformation) from a
    set of covariances/counts.  covfile is a Sphinx-III format matrix
    file as generated by lda_train, containing one covariance matrix
    per class, while counts is a text file with the observation counts
    of each class, one per line."""
    def __init__(self, covfile, countfile):
        self.cov = s3lda.open(covfile).getall()
        fh = open(countfile)
        self.count = map(lambda x:int(x.rstrip()), fh)
        self.totalcount = sum(self.count)
        fh.close()

    def objective(self, A, r, c):
        """Log-likelihood function for MLLT:
        N|A| - \sum_j \frac{N_j}{2} \log |diag(A \Sigma_j A^T)|"""
        # Note: A has been flattened to make it acceptable to scipy.optimize
        A = A.reshape((r,c))
        detA = det(A)
        ll = self.totalcount * log(detA)
        for j, nj in enumerate(self.count):
            C = self.cov[j]
            cl = diag(dot(dot(A, C), A.T))
            ll = ll - (float(nj) / 2) * log(prod(cl))
        print "likelihood: %f" % ll
        # Note: we negate this to maximize likelihood
        return -ll

    def gradient(self, A, r, c):
        """'Flattened' gradient of log-likelihood function for MLLT:
        N(A^T)^{-1} - \sum_j N_j diag(A \Sigma_j A^T)^{-1}A\Sigma_j"""
        # Note: A has been flattened to make it acceptable to scipy.optimize
        A = A.reshape((r,c))
        detA = det(A)
        lg = self.totalcount * inv(A.T)
        for j, nj in enumerate(self.count):
            C = self.cov[j]
            cl = diag(dot(dot(A, C), A.T))
            lg = lg - float(nj) * dot(dot(inv(diag(cl)), A), C)
        # Flatten out the gradient
        lg = lg.ravel()
        # This is a HACK!  The gradient can be extremely large which
        # causes line search to fail, or causes A to become
        # non-positive-definite.  So we scale it down arbitrarily.
        # Note: we negate this to maximize likelihood
        print lg
        return -lg * 1e-7

    def train(self, A=None):
        """Train an MLLT transform from an optional starting point."""
        if A == None:
            # Initialize it with a random positive-definite matrix of
            # the same shape as the covariances
            s = self.cov[0].shape
            d = -1
            while d < 0:
                A = eye(s[0]) + 0.1 * random(s)
                d = det(A)
            
        # Flatten out the matrix so scipy.optimize can handle it
        AA = fmin_bfgs(self.objective, A.ravel(), self.gradient, A.shape, disp=1)
        # And unflatten the maximum-likelihood
        return AA.reshape(A.shape)
